from aif360.datasets import StandardDataset
from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric

import pandas as pd
import numpy as np
from sklearn.metrics import average_precision_score, accuracy_score, f1_score, precision_score, recall_score

# Evaluation with bootstrapping
np.random.seed(1234)
rng = np.random.RandomState(1234)


def prepare_dataset_aif360(protected_attribute, model_predictions):
    '''returns a df of true labels,protected attribute and a df of predicted labels'''

    labels = model_predictions['y_true'].reset_index(drop=True)  # ground truth labels
    predictions = model_predictions['y_pred'].reset_index(drop=True)  # predicted labels
    attribute_column = model_predictions[protected_attribute]

    # Concatenate
    true_values = pd.concat([labels, attribute_column], axis=1)

    # drop nans 
    nan_idx = true_values.loc[pd.isna(true_values[protected_attribute]), :].index
    true_values = true_values[true_values[protected_attribute].notna()]
    predictions.drop(nan_idx, inplace=True)

    # convert protected attribute from boolean to int
    # true_values[protected_attribute] = true_values[protected_attribute].astype(int)

    # reset index
    true_values = true_values.reset_index(drop=True)
    predictions = predictions.reset_index(drop=True)
    return true_values, predictions



def get_df_per_attr (protected_attribute, df, dataset):
    '''returns a df of protected_attribute, subject, y_true, y_pred'''
    
    labels = df['y_true'].reset_index(drop=True)  # ground truth labels
    predictions = df['y_pred'].reset_index(drop=True)  # predicted labels
    attribute_column = df[protected_attribute]
    if dataset=='MIMIC':       
        subjects = df['subject'].reset_index(drop=True)  #ids
    if dataset=='GLOBEM':    
        subjects = df['PID'].reset_index(drop=True)  #ids
        
    # Concatenate
    df_per_attr = pd.concat([attribute_column,subjects, labels, predictions], axis=1)

    # drop nans 
    df_per_attr = df_per_attr[df_per_attr[protected_attribute].notna()]


    # reset index
    df_per_attr = df_per_attr.reset_index(drop=True)

    return df_per_attr


def aif360_model(true_values, predictions, protected_attribute, privileged_class, favorable_class=1):
    # An aif360 library class
    dataset = StandardDataset(true_values,  # the real values dataset
                              label_name='y_true',
                              favorable_classes=[favorable_class],
                              protected_attribute_names=[protected_attribute],
                              privileged_classes=[privileged_class])

    # Dataset containing predictions
    dataset_pred = dataset.copy()
    dataset_pred.labels = predictions
    return get_aif360_metrics(dataset, dataset_pred)



def print_aif360_result(metric_pred, classified_metric):
    # evaluating in terms of fairness
    # definitions = https://aif360.mybluemix.net/ - keep only disparate impact ratio here for testing
    result = {
        'statistical_parity_difference': metric_pred.statistical_parity_difference(),  # The difference of the
        # rate of favorable outcomes received by the unprivileged group to the privileged group (WAE view)
        'disparate_impact': metric_pred.disparate_impact(),  # the proportion of the unprivileged group that
        # received the positive outcome divided by the proportion of the privileged group that received the
        # positive outcome (WAE view)
        'equal_opportunity_difference': classified_metric.equal_opportunity_difference(),  # The difference of
        # true positive rates between the unprivileged and the privileged groups (between the two worldviews,
        # i.e., WYSIWYG and WAE).
        'average_absolute_odds_difference': classified_metric.average_abs_odds_difference(),  # fair near 0
        'error_rate_difference': classified_metric.error_rate_difference(),
        'error_rate_ratio': classified_metric.error_rate_ratio(),
        'false_discovery_rate_ratio': classified_metric.false_discovery_rate_ratio(),
        'false_negative_rate_ratio': classified_metric.false_negative_rate_ratio(),
        'false_omission_rate_ratio': classified_metric.false_omission_rate_ratio(),
        'false_positive_rate_ratio': classified_metric.false_positive_rate_ratio(),
        # 'performance_measures': classified_metric.performance_measures(privileged=True),
        'true_positive_rate_difference': classified_metric.true_positive_rate_difference(),
        #'average_predictive_value_difference': classified_metric.average_predictive_value_difference()
    }

    for r in result.keys():
        print("{}: {}".format(r, result.get(r)))
    return result


def get_aif360_metrics(dataset, dataset_pred):
    # for attr in dataset_pred.protected_attribute_names:
    attr = dataset_pred.protected_attribute_names[0] #the name of the attribute

    idx = dataset_pred.protected_attribute_names.index(attr) #he indices of the protected attribute attr
    privileged_groups = [{attr: dataset_pred.privileged_protected_attributes[idx][0]}]
    unprivileged_groups = [{attr: dataset_pred.unprivileged_protected_attributes[idx][0]}]

    classified_metric = ClassificationMetric(dataset, dataset_pred, unprivileged_groups=unprivileged_groups,
                                             privileged_groups=privileged_groups)

    metric_pred = BinaryLabelDatasetMetric(dataset_pred, unprivileged_groups=unprivileged_groups,
                                           privileged_groups=privileged_groups)

    return metric_pred, classified_metric


def print_aif360_accuracy_metrics(classified_metric):
    print("Privileged Group: {}".format(classified_metric.binary_confusion_matrix(privileged=True)))
    print("Unprivileged Group: {}".format(classified_metric.binary_confusion_matrix(privileged=False)))
    print()
    print("Overall Accuracy: {}".format(classified_metric.accuracy()))
    print("Privileged Group: {}".format(classified_metric.accuracy(privileged=True)))
    print("Unprivileged Group: {}".format(classified_metric.accuracy(privileged=False)))
    print()
    print("Overall Error Rate: {}".format(classified_metric.error_rate()))
    print("Privileged Group: {}".format(classified_metric.error_rate(privileged=True)))
    print("Unprivileged Group: {}".format(classified_metric.error_rate(privileged=False)))


# https://sites.google.com/site/lisaywtang/tech/python/scikit/auc-conf-interval
def get_ci_auc(y_true, y_pred, metric='roc'):
    from scipy.stats import sem
    from sklearn.metrics import roc_auc_score
    n_bootstraps = 500
    bootstrapped_scores = []
    for i in range(n_bootstraps):
        # bootstrap by sampling with replacement on the prediction indices
        indices = rng.random_integers(0, len(y_pred) - 1, len(y_pred))
        # indices = rng.randint(0, len(y_pred) + 1)

        if len(np.unique(y_true[indices])) < 2:
            # We need at least one positive and one negative sample for ROC AUC
            # to be defined: reject the sample
            continue

        if metric == 'AUROC':
            score = roc_auc_score(y_true[indices], y_pred[indices], average='micro')
        elif metric == 'AUPRC Micro':
            score = average_precision_score(y_true[indices], y_pred[indices], average='micro')
        elif metric == 'AUPRC Macro':
            score = average_precision_score(y_true[indices], y_pred[indices], average='macro')
        elif metric == 'Accuracy':
            score = accuracy_score(np.argmax(y_true[indices], axis=1), np.argmax(y_pred[indices], axis=1))
        elif metric == 'F1 Macro':
            score = f1_score(np.argmax(y_true[indices], axis=1), np.argmax(y_pred[indices], axis=1), average='macro')
        elif metric == 'F1 Micro':
            score = f1_score(np.argmax(y_true[indices], axis=1), np.argmax(y_pred[indices], axis=1), average='micro')
        elif metric == 'F1 Weighted':
            score = f1_score(np.argmax(y_true[indices], axis=1), np.argmax(y_pred[indices], axis=1), average='weighted')
        elif metric == 'Precision':
            score = precision_score(np.argmax(y_true[indices], axis=1), np.argmax(y_pred[indices], axis=1), average='macro')
        elif metric == 'Recall':
            score = recall_score(np.argmax(y_true[indices], axis=1), np.argmax(y_pred[indices], axis=1), average='macro')
        else:
            raise ValueError('Please provide correct metric parameter.')

        bootstrapped_scores.append(score)

    sorted_scores = np.array(bootstrapped_scores)
    sorted_scores.sort()
    confidence_lower = sorted_scores[int(0.025 * len(sorted_scores))]
    confidence_upper = sorted_scores[int(0.975 * len(sorted_scores))]

    return confidence_lower, confidence_upper
